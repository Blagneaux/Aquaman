import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Exponentiation, ExpSineSquared

# Build toy data in 3D space
x1 = np.linspace(start=1000, stop=10000, num=10)
x2 = np.linspace(start=6, stop=60, num=55)
X1, X2 = np.meshgrid(x1, x2)
X = np.column_stack([X1.ravel(), X2.ravel()])
# y = np.squeeze(X[:, 0] * np.sin(X[:, 1]))
y1 = -np.log10(x1)+3.71
y2 = 7.69*np.log10(x2/6)-0.87
Y1, Y2 = np.meshgrid(y1*10, y2)
y = np.column_stack([Y1.ravel(), Y2.ravel()])

# ------------------------------------------------------------
# Read the observation file generated by Liypad
# ------------------------------------------------------------

# Initialize std_prediction for the first loop
std_prediction = None

# Random state
rng = np.random.RandomState(1)
count = 10
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(211, projection='3d')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('f(X1, X2)')
ax2 = fig.add_subplot(212, projection='3d')
ax2.set_xlabel('X1')
ax2.set_ylabel('X2')
ax2.set_zlabel('f(X1, X2)')
plt.tight_layout()

while True:
    ax.clear()
    ax2.clear()
    count += 1
    # Select experimental measurements from toy data
    if std_prediction is not None:
        new_index = np.argmax([np.linalg.norm(s) for s in std_prediction])
        if new_index not in training_indices:
            training_indices = np.append(training_indices, new_index)
            # ------------------------------------------------------------
            # Write a file with the coordinates of the next observation, to use it as input for Lilypad
            # ------------------------------------------------------------
    else:
        training_indices = rng.choice(np.arange(200), size=10, replace=False)

    X_train, y_train = X[training_indices], y[training_indices]

    print(training_indices)

    # Add some noise
    noise_std = 0.75
    y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)

    # Create the Gaussian process model
    # kernel = 1 * RBF(length_scale=np.array([1000, 1]), length_scale_bounds=(1, 10))
    kernel = Exponentiation(1 * RBF(length_scale=np.array([1000, 1]), length_scale_bounds=(1, 10)), exponent=4)
    gaussian_process = GaussianProcessRegressor(
        kernel=kernel, n_restarts_optimizer=9
    )
    gaussian_process.fit(X_train, y_train_noisy)
    mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)

    # Plot the result in 3D
    ax.scatter(X_train[:, 0], X_train[:, 1], y_train_noisy[:, 0], color='blue', label='Observations')
    ax.plot_trisurf(X[:, 0], X[:, 1], mean_prediction[:, 0], linewidth=0.2, antialiased=True, cmap='viridis', alpha=0.5, label='Mean prediction')
    ax.plot_surface(X1, X2, y[:, 0].reshape(X1.shape), color='tab:orange', alpha=0.3, label='Real surface')  # Adding real surface
    # plt.legend()
    ax2.scatter(X_train[:, 0], X_train[:, 1], y_train_noisy[:, 1], color='blue', label='Observations')
    ax2.plot_trisurf(X[:, 0], X[:, 1], mean_prediction[:, 1], linewidth=0.2, antialiased=True, cmap='viridis', alpha=0.5, label='Mean prediction')
    ax2.plot_surface(X1, X2, y[:, 1].reshape(X1.shape), color='tab:orange', alpha=0.3, label='Real surface')  # Adding real surface
    # plt.legend()
    plt.pause(0.1)

    if count%100 == 0:
        # Ask if the user wants to continue
        cont = input("Do you want to continue? (y/n): ")
        if cont.lower() != 'y':
            break
